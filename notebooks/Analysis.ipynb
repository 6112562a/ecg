{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/Users/rajpurkar/Documents/Code/ecg\")\n",
    "sys.path.append('./ecg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_folder = 'saved/predictions/1503712028'\n",
    "\n",
    "import json\n",
    "import evaluate\n",
    "import predict\n",
    "\n",
    "x, gt, probs, processor = predict.load_predictions(prediction_folder)\n",
    "params = json.load(open(prediction_folder + '/params.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = params[\"data_path\"]\n",
    "ext = '*_grp*.episodes.json'\n",
    "\n",
    "def get_files(path):\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for filename in fnmatch.filter(filenames, ext):\n",
    "            yield(os.path.join(root, filename))\n",
    "\n",
    "def patient_id(record):\n",
    "    return os.path.basename(record).split(\"_\")[0]\n",
    "\n",
    "class_patients = defaultdict(set)\n",
    "for f in tqdm(get_files(path)):\n",
    "    jfile = json.load(open(f, 'r'))\n",
    "    for episode in jfile['episodes']:\n",
    "        rhythm_name = episode['rhythm_name']\n",
    "        class_patients[rhythm_name].add(patient_id(f))\n",
    "\n",
    "for (k, v) in class_patients.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for (k, v) in sorted(class_patients.items()):\n",
    "    lv = len(v)\n",
    "    total += lv\n",
    "    print(k, lv)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truths = gt\n",
    "num_outputs_for_thirty_seconds = len(truths[0])\n",
    "truths_flat = truths.flatten()\n",
    "classes_unique, counts = np.unique(truths_flat, return_counts=True)\n",
    "classes_u = np.array(processor.classes)[classes_unique]\n",
    "num_hours = (counts * 30 / num_outputs_for_thirty_seconds) / 3600.0\n",
    "for pair in zip(classes_u, num_hours):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x\n",
    "y = gt[0]\n",
    "\n",
    "def from_one_hot_to_int(label):\n",
    "    return np.argmax(label, axis=-1)\n",
    "\n",
    "def get_x_y_predictions_at_index(index, probs=None):\n",
    "    x_sample = x[index]\n",
    "    y_sample = y[index]\n",
    "    y_prediction = None\n",
    "    if probs is not None:\n",
    "        y_prediction = np.argmax(probs[index], axis=-1)\n",
    "    return x_sample, y_sample, y_prediction\n",
    "\n",
    "def get_sample_from_classes(\n",
    "        categories, min_mistakes = 20, num_tries = 1000, only_classes=False, probs=None):\n",
    "    class_nums = np.array([processor.classes.index(c) for c in categories])\n",
    "    indices = np.where(np.array([np.in1d(class_nums, row).all() for row in y]))[0]\n",
    "    for _ in range(num_tries):\n",
    "        index = random.choice(indices)\n",
    "        y_prediction = None\n",
    "        x_sample, y_sample, y_prediction = get_x_y_predictions_at_index(index, probs=probs)\n",
    "        if only_classes:\n",
    "            if (set(np.unique(y_sample)) != set(np.unique(class_nums))):\n",
    "                continue\n",
    "        num_wrong = 0\n",
    "        if y_prediction is None:\n",
    "            break\n",
    "        num_wrong = np.sum(y_sample != y_prediction)\n",
    "        if (num_wrong > min_mistakes):\n",
    "            print(\"Prediction got wrong \" +  str(num_wrong * 1.0 / len(y_sample)))\n",
    "            break\n",
    "    return x_sample, y_sample, y_prediction, index\n",
    "\n",
    "#for class_indiv in processor.classes:\n",
    "x_sample, y_sample, y_prediction, index = get_sample_from_classes([u'SINUS'], only_classes=False, probs=probs, min_mistakes=0)\n",
    "#x_sample, y_sample, y_prediction = get_x_y_predictions_at_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "from itertools import groupby\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 6)\n",
    "\n",
    "def draw_sample(x_sample, y_sample, y_prediction, step, show_label=True, save=False, small_frame=False):\n",
    "    colors = cm.Pastel2(np.linspace(0, 1, 20))\n",
    "    y_times = np.linspace(step/2, len(x_sample) - step/2, len(y_sample))\n",
    "    if show_label is True:\n",
    "        grouped_labels = [(k, sum(1 for i in g)) for k,g in groupby(y_sample)]\n",
    "        acc = 0\n",
    "        seen = {}\n",
    "        for label, number in grouped_labels:\n",
    "            p = {\n",
    "                \"color\": colors[label],\n",
    "                \"alpha\": 0.5,\n",
    "                \"lw\": 0\n",
    "            }\n",
    "            if label not in seen:\n",
    "                label_name = processor.int_to_class[label]\n",
    "                p[\"label\"] = label_name\n",
    "                seen[label] = True\n",
    "            plt.axvspan(\n",
    "                acc * step,\n",
    "                (acc + number) * step, **p)\n",
    "            acc += number\n",
    "    print(np.array(processor.classes)[y_sample])\n",
    "    if y_prediction is not None:\n",
    "        print(np.array(processor.classes)[y_prediction])\n",
    "    plt.plot(x_sample, color='#000000', alpha=1)\n",
    "    plt.legend(loc=\"best\", prop={'size':14})\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.tight_layout()\n",
    "    if small_frame is True:\n",
    "        plt.xlim([1050, 3050])\n",
    "    if save is True:\n",
    "        plt.savefig(str(np.unique(np.array(classes)[y_sample])[0]) + \"-\" + str(index) + '.pdf', dpi=400, format='pdf',bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "step = params[\"step\"] if \"step\" in params else 256\n",
    "draw_sample(x_sample, y_sample, y_prediction, step, save=False, show_label=True, small_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import human_performance\n",
    "from tabulate import tabulate\n",
    "import load\n",
    "\n",
    "aggregate_data = []\n",
    "f1_data = []\n",
    "for metric in ['seq', 'set']:\n",
    "    # models\n",
    "    evaluator = evaluate.evaluate_multiclass(\n",
    "        gt, probs, processor.classes, metric, ', '.join(params[\"model_paths\"]), display_scores=False)\n",
    "    model_plotMat, model_support, class_names = evaluate.parse_classification_report(evaluator.scorer.report)\n",
    "    model_f1 = model_plotMat[:-1, 2]\n",
    "    f1_data.append(model_f1)\n",
    "    aggregate_data.append(model_plotMat[-1, :])\n",
    "\n",
    "    # humans\n",
    "    params_copy = params.copy()\n",
    "    human_ground_truths, human_probs = human_performance.human_gt_and_probs(params_copy, x, gt, processor)\n",
    "    evaluator = evaluate.evaluate_multiclass(\n",
    "        human_ground_truths, human_probs, processor.classes, metric, ', '.join(params[\"model_paths\"]), display_scores=False)\n",
    "    human_plotMat, human_support, class_names = evaluate.parse_classification_report(evaluator.scorer.report)\n",
    "    human_f1 = human_plotMat[:-1, 2]\n",
    "    f1_data.append(human_f1)\n",
    "    aggregate_data.append(human_plotMat[-1, :])\n",
    "\n",
    "cell_text = []\n",
    "\n",
    "f1_data = np.array(f1_data).T\n",
    "for row, class_name in zip(f1_data, class_names):\n",
    "    cell_text.append([class_name] + ['%1.3f' % x for x in row])\n",
    "\n",
    "aggregate_data = np.array(aggregate_data).T\n",
    "for row, class_name in zip(aggregate_data, ['Precision', 'Recall', 'F1']):\n",
    "    cell_text.append([class_name] + ['%1.3f' % x for x in row])\n",
    "\n",
    "table = tabulate(\n",
    "    cell_text, tablefmt=\"latex\", floatfmt=\".3f\",\n",
    "    headers=[\"Model seq\", \"Human seq\", \"Model set\", \"Human set\"])\n",
    "\n",
    "rows = []\n",
    "import re\n",
    "for row in table.split('\\n'):\n",
    "    elems = re.split('\\s+', row)\n",
    "    if len(elems) > 2 and \"seq\" not in elems:\n",
    "        for start in [3, 7]:\n",
    "            end = start+2\n",
    "            winner = start if float(elems[start]) > float(elems[end]) else end\n",
    "            elems[winner] = \"\\\\textbf{\" + elems[winner] + \"}\"\n",
    "    row = \" \".join(elems)\n",
    "    rows.append(row)\n",
    "table = \"\\n\".join(rows)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# establish difference in scores, not neccessarily show that it's better\n",
    "import human_performance\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.MultiLabelBinarizer(range(len(processor.classes)))\n",
    "\n",
    "def get_preds_from_probs(probs):\n",
    "    preds = np.argmax(probs, axis=-1)\n",
    "    return preds\n",
    "\n",
    "# in case we want to do binary\n",
    "def get_binary_preds_from_probs(probs):\n",
    "    preds = get_preds_from_probs(probs)\n",
    "    multi_label_preds = lb.fit_transform(preds)\n",
    "    return multi_label_preds\n",
    "\n",
    "test_params_copy = params.copy()\n",
    "human_probs_all = None\n",
    "human_probs_concat = []\n",
    "\n",
    "for i in [0,1,2,3,4,5]:\n",
    "    test_params_copy[\"epi_ext\"] = \"_rev\" + str(i) + \".episodes.json\"\n",
    "    _, human_probs, _ = load.load_x_y_with_processor(test_params_copy, processor)\n",
    "    human_probs_concat.append(human_probs)\n",
    "    if human_probs_all is None:\n",
    "        human_probs_all = human_probs\n",
    "    else:\n",
    "        human_probs_all = human_probs + human_probs_all\n",
    "human_probs_concat = np.array(human_probs_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import evaluate\n",
    "\n",
    "def get_accs(preds, gt):\n",
    "    return np.count_nonzero(preds == gt, axis=-1)[0]*1.0 / preds.shape[1]\n",
    "\n",
    "def get_ttest_from_accs(hb_accs, mb_accs, title):\n",
    "    #print(np.sum(hb_accs - mb_accs > 0))\n",
    "    #print(np.sum(mb_accs - hb_accs > 0))\n",
    "\n",
    "    mean_hb_acc = np.mean(hb_accs)\n",
    "    score = ttest_rel(mb_accs, hb_accs)\n",
    "    return score, mean_hb_acc, hb_accs, mb_accs\n",
    "\n",
    "def get_ttest_from_probs(h_probs, m_probs, title='human'):\n",
    "    mb = get_preds_from_probs(m_probs)\n",
    "    mb_accs = get_accs(mb, gt)\n",
    "    hb = get_preds_from_probs(h_probs)\n",
    "    hb_accs = get_accs(hb, gt)\n",
    "    #evaluator = evaluate.evaluate_multiclass(gt, h_probs, processor.classes, 'seq', title, display_scores=True)\n",
    "    return get_ttest_from_accs(hb_accs, mb_accs, title=title)\n",
    "   \n",
    "\n",
    "cell_text = []\n",
    "\n",
    "hb_accs_concat = []\n",
    "for index, human_probs in enumerate(human_probs_concat):\n",
    "    score, mean_hb_acc, hb_accs, mb_accs = get_ttest_from_probs(human_probs, probs)\n",
    "    hb_accs_concat.append(hb_accs)\n",
    "    # cell_text.append(['Human ' + str(index + 1), mean_hb_acc, score.pvalue])\n",
    "\n",
    "# get average accuracy\n",
    "hb_accs_concat = np.array(hb_accs_concat)\n",
    "avg_hb_accs = np.mean(hb_accs_concat, axis = 0)\n",
    "score, mean, _, _ = get_ttest_from_accs(avg_hb_accs, mb_accs, title='avg human')\n",
    "cell_text.append(['Human Acc. Averaged', mean, score.pvalue])\n",
    "\n",
    "# majority vote\n",
    "maj_score, mean, _, _ = get_ttest_from_probs(human_probs_all, probs, title='maj vote')\n",
    "cell_text.append(['Majority Vote Human', mean, maj_score.pvalue])\n",
    "\n",
    "# model\n",
    "maj_score, mean, _, _ = get_ttest_from_probs(probs, probs, title='maj vote')\n",
    "cell_text.append(['Model', mean, maj_score.pvalue])\n",
    "\n",
    "table = tabulate(\n",
    "    cell_text, floatfmt=\".7f\",\n",
    "    headers=[\"Class\", \"Mean Acc\", \"Paired-t-test p-value\"])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
