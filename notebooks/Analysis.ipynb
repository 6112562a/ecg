{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/Users/rajpurkar/Documents/Code/ecg\")\n",
    "sys.path.append('./ecg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FROM_MODEL = True\n",
    "LOAD_FROM_FILE = False\n",
    "\n",
    "model_paths = ['saved/default/1498801603-612/0.430-0.849-013-0.313-0.883.hdf5', 'saved/default/1498801603-783/0.425-0.849-010-0.344-0.871.hdf5', 'saved/default/1498680570-155/0.426-0.848-011-0.339-0.873.hdf5']\n",
    "\n",
    "import load\n",
    "import json\n",
    "import util\n",
    "import predict\n",
    "\n",
    "if LOAD_FROM_MODEL is True:\n",
    "    params = util.get_model_params(model_paths[0])\n",
    "    test_params = json.load(open('./configs/test.json', 'r'))\n",
    "    x_gt, gt, processor, dl = load.load_test(\n",
    "            test_params,\n",
    "            train_params=params,\n",
    "            split='test')\n",
    "    model_probs = predict.get_ensemble_pred_probs(model_paths, x_gt)\n",
    "elif LOAD_FROM_FILE is True:\n",
    "    model_probs = None\n",
    "    #file_to_load = open('./configs/train.json', 'r')\n",
    "    file_to_load = open('./configs/test.json', 'r')\n",
    "    params = json.load(file_to_load)\n",
    "    dl, processor = load.load_train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = params[\"data_path\"]\n",
    "ext = '*_grp*.episodes.json'\n",
    "#ext = '*.episodes.json'\n",
    "\n",
    "def get_files(path):\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for filename in fnmatch.filter(filenames, ext):\n",
    "            yield(os.path.join(root, filename))\n",
    "\n",
    "def patient_id(record):\n",
    "    return os.path.basename(record).split(\"_\")[0]\n",
    "\n",
    "class_patients = defaultdict(set)\n",
    "for f in tqdm(get_files(path)):\n",
    "    jfile = json.load(open(f, 'r'))\n",
    "    for episode in jfile['episodes']:\n",
    "        rhythm_name = episode['rhythm_name']\n",
    "        if rhythm_name == 'SUDDEN_BRADY':\n",
    "            rhythm_name = u'CHB'\n",
    "        if rhythm_name == 'NSR':\n",
    "            rhythm_name = u'SINUS'\n",
    "        class_patients[rhythm_name].add(patient_id(f))\n",
    "\n",
    "for (k, v) in class_patients.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "for (k, v) in sorted(class_patients.items()):\n",
    "    lv = len(v)\n",
    "    total += lv\n",
    "    print(k, lv)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = dl.x_test\n",
    "y = dl.y_test\n",
    "\n",
    "def from_one_hot_to_int(label):\n",
    "    return np.argmax(label, axis=-1)\n",
    "\n",
    "def get_x_y_predictions_at_index(index, probs=None):\n",
    "    x_sample = x[index]\n",
    "    y_sample = from_one_hot_to_int(y[index])\n",
    "    y_prediction = None\n",
    "    if probs is not None:\n",
    "        y_prediction = np.argmax(probs[index], axis=-1)\n",
    "    return x_sample, y_sample, y_prediction\n",
    "\n",
    "def get_sample_from_classes(\n",
    "        categories, min_mistakes = 20, num_tries = 1000, only_classes=False, probs=None):\n",
    "    classes = np.array([processor.class_to_int[c] for c in categories])\n",
    "    y_maxed = np.argmax(y, axis=-1)\n",
    "    indices = np.where(np.array([np.in1d(classes, row).all() for row in y_maxed]))[0]\n",
    "    for _ in range(num_tries):\n",
    "        index = random.choice(indices)\n",
    "        y_prediction = None\n",
    "        x_sample, y_sample, y_prediction = get_x_y_predictions_at_index(index, probs=probs)\n",
    "        if only_classes:\n",
    "            if (set(np.unique(y_sample)) != set(np.unique(classes))):\n",
    "                continue\n",
    "        num_wrong = 0\n",
    "        if y_prediction is None:\n",
    "            break\n",
    "        num_wrong = np.sum(y_sample != y_prediction)\n",
    "        if (num_wrong > min_mistakes):\n",
    "            print(\"Prediction got wrong \" +  str(num_wrong * 1.0 / len(y_sample)))\n",
    "            break\n",
    "    return x_sample, y_sample, y_prediction, index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truths = np.argmax(y, axis=2)\n",
    "num_outputs_for_thirty_seconds = len(truths[0])\n",
    "truths_flat = truths.flatten()\n",
    "classes_unique, counts = np.unique(truths_flat, return_counts=True)\n",
    "classes_u = np.array(processor.classes)[classes_unique]\n",
    "num_hours = (counts * 30 / num_outputs_for_thirty_seconds) / 3600.0\n",
    "for pair in zip(classes_u, num_hours):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "from itertools import groupby\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 6)\n",
    "\n",
    "def from_int_to_name(l):\n",
    "    return processor.classes[l]\n",
    "\n",
    "def draw_sample(x_sample, y_sample, y_prediction, step, show_label=True, save=False, small_frame=False):\n",
    "    colors = cm.Pastel2(np.linspace(0, 1, 20))\n",
    "    y_times = np.linspace(step/2, len(x_sample) - step/2, len(y_sample))\n",
    "    if show_label is True:\n",
    "        grouped_labels = [(k, sum(1 for i in g)) for k,g in groupby(y_sample)]\n",
    "        acc = 0\n",
    "        seen = {}\n",
    "        for label, number in grouped_labels:\n",
    "            p = {\n",
    "                \"color\": colors[label],\n",
    "                \"alpha\": 0.5,\n",
    "                \"lw\": 0\n",
    "            }\n",
    "            if label not in seen:\n",
    "                label_name = from_int_to_name(label)\n",
    "                p[\"label\"] = label_name\n",
    "                seen[label] = True\n",
    "            plt.axvspan(\n",
    "                acc * step,\n",
    "                (acc + number) * step, **p)\n",
    "            acc += number\n",
    "    print(np.array(processor.classes)[y_sample])\n",
    "    if y_prediction is not None:\n",
    "        print(np.array(processor.classes)[y_prediction])\n",
    "    plt.plot(x_sample, color='#000000', alpha=1)\n",
    "    plt.legend(loc=\"best\", prop={'size':14})\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.tight_layout()\n",
    "    if small_frame is True:\n",
    "        plt.xlim([1050, 3050])\n",
    "    if save is True:\n",
    "        plt.savefig(str(np.unique(np.array(processor.classes)[y_sample])[0]) + \"-\" + str(index) + '.pdf', dpi=400, format='pdf',bbox_inches='tight',pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "#for class_indiv in processor.classes:\n",
    "x_sample, y_sample, y_prediction, index = get_sample_from_classes([u'AFIB', u'SINUS'], only_classes=False, probs=model_probs, min_mistakes=0)\n",
    "\n",
    "#index = 161 # good afib, sinus\n",
    "#index = 33 # good afib, sinus\n",
    "#index = 275 # good afib, sinus\n",
    "index = 169 # good afib, sinus\n",
    "x_sample, y_sample, y_prediction = get_x_y_predictions_at_index(index)\n",
    "step = params[\"step\"] if \"step\" in params else 256\n",
    "draw_sample(x_sample, y_sample, y_prediction, step, save=True, show_label=False, small_frame=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import human_performance\n",
    "from tabulate import tabulate\n",
    "\n",
    "aggregate_data = []\n",
    "f1_data = []\n",
    "for metric in ['seq', 'set']:\n",
    "    # models\n",
    "    evaluator = evaluate.evaluate_multiclass(\n",
    "        gt, model_probs, processor.classes, metric, ', '.join(model_paths), display_scores=False)\n",
    "    model_plotMat, model_support, class_names = evaluate.parse_classification_report(evaluator.scorer.report)\n",
    "    model_f1 = model_plotMat[:-1, 2]\n",
    "    f1_data.append(model_f1)\n",
    "    aggregate_data.append(model_plotMat[-1, :])\n",
    "\n",
    "    # humans\n",
    "    test_params_copy = test_params.copy()\n",
    "    human_ground_truths, human_probs = human_performance.human_gt_and_probs(test_params_copy, x_gt, gt, processor)\n",
    "    evaluator = evaluate.evaluate_multiclass(\n",
    "        human_ground_truths, human_probs, processor.classes, metric, ', '.join(model_paths), display_scores=False)\n",
    "    human_plotMat, human_support, class_names = evaluate.parse_classification_report(evaluator.scorer.report)\n",
    "    human_f1 = human_plotMat[:-1, 2]\n",
    "    f1_data.append(human_f1)\n",
    "    aggregate_data.append(human_plotMat[-1, :])\n",
    "\n",
    "cell_text = []\n",
    "\n",
    "f1_data = np.array(f1_data).T\n",
    "for row, class_name in zip(f1_data, class_names):\n",
    "    cell_text.append([class_name] + ['%1.3f' % x for x in row])\n",
    "\n",
    "aggregate_data = np.array(aggregate_data).T\n",
    "for row, class_name in zip(aggregate_data, ['Precision', 'Recall', 'F1']):\n",
    "    cell_text.append([class_name] + ['%1.3f' % x for x in row])\n",
    "\n",
    "table = tabulate(\n",
    "    cell_text, tablefmt=\"latex\", floatfmt=\".3f\",\n",
    "    headers=[\"Model seq\", \"Human seq\", \"Model set\", \"Human set\"])\n",
    "\n",
    "rows = []\n",
    "import re\n",
    "for row in table.split('\\n'):\n",
    "    elems = re.split('\\s+', row)\n",
    "    if len(elems) > 2 and \"seq\" not in elems:\n",
    "        for start in [3, 7]:\n",
    "            end = start+2\n",
    "            winner = start if float(elems[start]) > float(elems[end]) else end\n",
    "            elems[winner] = \"\\\\textbf{\" + elems[winner] + \"}\"\n",
    "    row = \" \".join(elems)\n",
    "    rows.append(row)\n",
    "table = \"\\n\".join(rows)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 *binom.cdf(5,19, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import human_performance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from scipy.stats import binom\n",
    "from sklearn import preprocessing\n",
    "from tabulate import tabulate\n",
    "\n",
    "lb = preprocessing.MultiLabelBinarizer(range(len(processor.classes)))\n",
    "\n",
    "def get_binary_preds_from_probs(probs):\n",
    "    preds = np.argmax(probs, axis=-1)\n",
    "    multi_label_preds = lb.fit_transform(preds)\n",
    "    return multi_label_preds\n",
    "\n",
    "test_params_copy = test_params.copy()\n",
    "human_probs_all = None\n",
    "for i in [0,1,2,3,5]:\n",
    "    test_params_copy[\"epi_ext\"] = \"_rev\" + str(i) + \".episodes.json\"\n",
    "    _, human_probs, _ = load.load_x_y_with_processor(test_params_copy, processor)\n",
    "    if human_probs_all is None:\n",
    "        human_probs_all = human_probs\n",
    "    else:\n",
    "        human_probs_all = human_probs + human_probs_all\n",
    "\n",
    "hb = get_binary_preds_from_probs(human_probs_all)\n",
    "mb = get_binary_preds_from_probs(model_probs)\n",
    "\n",
    "\n",
    "p_values = []\n",
    "for class_int, class_label in enumerate(processor.classes):\n",
    "    cnf = confusion_matrix(hb[:, class_int], mb[:, class_int])\n",
    "    if cnf.shape[0] != 2: continue\n",
    "    y01 = cnf[0, 1]\n",
    "    y10 = cnf[1, 0]\n",
    "    McN = (np.abs(y01-y10))**2 / (y10+y01)\n",
    "    if y01 + y10 >= 25:\n",
    "        p_value = stats.chi2.sf(McN,1)\n",
    "    else:\n",
    "        p_value = 2 * binom.cdf(min(y01, y10), y10 + y01, 0.5)\n",
    "    # print(class_label, cnf, '%.16f' % p_value)\n",
    "    p_values.append((p_value, class_label))\n",
    "\n",
    "alpha = 0.01\n",
    "n = len(p_values)\n",
    "\n",
    "\n",
    "cell_text = []\n",
    "\n",
    "for index, (p_value, class_label) in enumerate(sorted(p_values)):\n",
    "    threshold = (index + 1) * alpha / n\n",
    "    reject_null = (p_value <= threshold)\n",
    "    cell_text.append([class_label, '%.5f' % p_value, reject_null])\n",
    "\n",
    "    \n",
    "table = tabulate(\n",
    "    cell_text, floatfmt=\".5f\",\n",
    "    headers=[\"Class\", \"P Value\", \"Reject H0\"])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
