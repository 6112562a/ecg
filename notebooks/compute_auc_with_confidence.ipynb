{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/afs/cs.stanford.edu/u/awni/scr/ecg-master/ecg\")\n",
    "import predict\n",
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_folder = \"/deep/u/pranavsr/saved/predictions/1503713063/\"\n",
    "x, gt, probs, processor = predict.load_predictions(prediction_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def c_statistic_with_95p_confidence_interval(cstat, num_positives, num_negatives, z_alpha_2=1.96):\n",
    "    \"\"\"\n",
    "    Calculates the confidence interval of an ROC curve (c-statistic), using the method described\n",
    "    under \"Confidence Interval for AUC\" here:\n",
    "      https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_the_Area_Under_an_ROC_Curve.pdf\n",
    "    Args:\n",
    "        cstat: the c-statistic (equivalent to area under the ROC curve)\n",
    "        num_positives: number of positive examples in the set.\n",
    "        num_negatives: number of negative examples in the set.\n",
    "        z_alpha_2 (optional): the critical value for an N% confidence interval, e.g., 1.96 for 95%,\n",
    "            2.326 for 98%, 2.576 for 99%, etc.\n",
    "    Returns:\n",
    "        The 95% confidence interval half-width, e.g., the Y in X Â± Y.\n",
    "    \"\"\"\n",
    "    q1 = cstat / (2 - cstat)\n",
    "    q2 = 2 * cstat**2 / (1 + cstat)\n",
    "    numerator = cstat * (1 - cstat) \\\n",
    "        + (num_positives - 1) * (q1 - cstat**2) \\\n",
    "        + (num_negatives - 1) * (q2 - cstat**2)\n",
    "    standard_error_auc = math.sqrt(numerator / (num_positives * num_negatives))\n",
    "    return z_alpha_2 * standard_error_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Scores (sequence level)\n",
      "AF      \t0.975\t0.968\t0.982\n",
      "AVB     \t0.989\t0.984\t0.994\n",
      "BIGEMINY\t0.998\t0.995\t1.002\n",
      "EAR     \t0.908\t0.883\t0.932\n",
      "IVR     \t0.995\t0.987\t1.002\n",
      "JUNCTIONAL\t0.985\t0.978\t0.992\n",
      "NOISE   \t0.985\t0.979\t0.992\n",
      "SINUS   \t0.976\t0.973\t0.980\n",
      "SVT     \t0.972\t0.959\t0.985\n",
      "TRIGEMINY\t0.999\t0.995\t1.002\n",
      "VT      \t0.995\t0.981\t1.009\n",
      "WENCKEBACH\t0.982\t0.972\t0.991\n",
      "0.978633555005\n"
     ]
    }
   ],
   "source": [
    "gts = gt.squeeze()\n",
    "\n",
    "def roc_auc(gts, probs, index):\n",
    "    n_gts = np.zeros_like(gts)\n",
    "    n_gts[gts==index] = 1\n",
    "    n_pos = np.sum(n_gts == 1)\n",
    "    n_neg = n_gts.size - n_pos\n",
    "    n_ps = probs[:,:,index].squeeze()\n",
    "    n_gts, n_ps = n_gts.ravel(), n_ps.ravel()\n",
    "    return n_pos, n_neg, skm.roc_auc_score(n_gts, n_ps)\n",
    "\n",
    "print \"AUC Scores (sequence level)\"\n",
    "macro_average = 0.0; total = 0.0\n",
    "for idx, cname in processor.int_to_class.items():\n",
    "    pos, neg, auc = roc_auc(gts, probs, idx)\n",
    "    total += pos\n",
    "    macro_average += pos * auc\n",
    "    conf = c_statistic_with_95p_confidence_interval(auc, pos, neg)\n",
    "    print \"{: <8}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(cname, auc, auc-conf,auc+conf)\n",
    "print \"Average\", macro_average / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Scores (set level)\n",
      "AF      \t0.959\t0.924\t0.994\n",
      "AVB     \t0.981\t0.954\t1.000\n",
      "BIGEMINY\t0.997\t0.981\t1.000\n",
      "EAR     \t0.935\t0.863\t1.000\n",
      "IVR     \t0.986\t0.958\t1.000\n",
      "JUNCTIONAL\t0.980\t0.949\t1.000\n",
      "NOISE   \t0.958\t0.914\t1.000\n",
      "SINUS   \t0.981\t0.968\t0.994\n",
      "SVT     \t0.940\t0.884\t0.996\n",
      "TRIGEMINY\t0.997\t0.979\t1.000\n",
      "VT      \t0.981\t0.935\t1.000\n",
      "WENCKEBACH\t0.981\t0.948\t1.000\n",
      "Average 0.974487907945\n"
     ]
    }
   ],
   "source": [
    "def roc_auc_set(gts, probs, index):\n",
    "    max_ps = np.max(probs[...,index], axis=1)\n",
    "    max_gts = np.any(gts==index, axis=1)\n",
    "    pos = np.sum(max_gts)\n",
    "    neg = max_gts.size - pos\n",
    "    return pos, neg, skm.roc_auc_score(max_gts, max_ps)\n",
    "\n",
    "print \"AUC Scores (set level)\"\n",
    "macro_average = 0.0; total = 0.0\n",
    "for idx, cname in processor.int_to_class.items():\n",
    "    pos, neg, auc = roc_auc_set(gts, probs, idx)\n",
    "    total += pos\n",
    "    macro_average += pos * auc\n",
    "    conf = c_statistic_with_95p_confidence_interval(auc, pos, neg)\n",
    "    print \"{: <8}\\t{:.3f}\\t{:.3f}\\t{:.3f}\".format(cname, auc, auc-conf, min(1, auc+conf))\n",
    "print \"Average\", macro_average / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
